python ./run_language_modeling.py \
--output_dir=models/PTC_TAPT_n_RoBERTa \
--model_type=roberta \
--block_size=128 \
--max_position_embeddings=128 \
--overwrite_output_dir \
--model_name_or_path=roberta-large \
--train_data_file=../datasets/train-articles/all_articles.txt \
--eval_data_file=../datasets/dev-articles/article730081389.txt \
--mlm \
--line_by_line \
--Ngram_path=output/ngrams/PTCtrain_ngrams.txt \
--num_train_epochs 3.0 \
--fasttext_model_path=output/ngrams/PTCtrain_ngrams.npy \
--learning_rate 2e-5 \
--no_cuda \
--per_device_train_batch_size=8 \
--per_device_eval_batch_size=8
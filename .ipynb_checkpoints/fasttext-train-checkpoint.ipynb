{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80314b12",
   "metadata": {},
   "source": [
    "# Data prep for T-DNA\n",
    "https://github.com/shizhediao/T-DNA\n",
    "<img src=\"model-training-fine-tuning.png\" width=\"50%\">\n",
    "\n",
    "1. fasttext model from which we get ngram embeddings\n",
    "2. T-DNA expects:\n",
    "    * data in the form of `text \\t label` - english_snippet_graph_matches_100k.tsv\n",
    "    * ngrams frequency file in the form `ngram \\t count` - english_snippet_graph_matches_100k_ngrams.tsv\n",
    "    * ngram embeddings file in numpy array format - english_snippet_graph_matches_100k_fasttext.npy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874a092f-24fc-4c0d-acd6-cf710d4712e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Using cached fasttext-0.9.2.tar.gz (68 kB)\n",
      "Collecting pybind11>=2.2\n",
      "  Using cached pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/khamilton/anaconda3/lib/python3.9/site-packages (from fasttext) (58.0.4)\n",
      "Requirement already satisfied: numpy in /home/khamilton/anaconda3/lib/python3.9/site-packages (from fasttext) (1.20.3)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=310321 sha256=dd473315fb548d6f0bd4bab942c5094aeeb65e3a88ce4432f15ad85f51597848\n",
      "  Stored in directory: /home/khamilton/.cache/pip/wheels/64/57/bc/1741406019061d5664914b070bd3e71f6244648732bc96109e\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.10.3\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b20083-9625-4763-918b-141c63c6bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "803a7621-5815-4edc-807e-8b02d93ec380",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../datasets/train-articles/all_articles.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "229f7647-45a0-4a89-8984-ec3d67b990aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17484 ../datasets/train-articles/all_articles.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce0d3bf6-c3a7-4519-b766-c1008013700e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  7463\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    7857 lr:  0.000000 avg.loss:  2.629731 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# train a model from the 'TAPT' data so we can extract warm-start embeddings for the ngrams to feed the T-DNA model training code.\n",
    "# the dimension of the vectors must be the same as the LLM we will be continuing to train. In this case roberta-large, which has dim=1024\n",
    "# since we will only be using unigrams and bigrams, we only need wordNgrams set to 2\n",
    "model = fasttext.train_unsupervised(data_path, \n",
    "                                    model='skipgram', \n",
    "                                    lr=0.05, \n",
    "                                    dim=1024, \n",
    "                                    ws=4, \n",
    "                                    wordNgrams=1, \n",
    "                                    epoch=3, \n",
    "                                    thread=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced67a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"models/PTC_1024_fasttext.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e226c0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model(\"models/PTC_1024_fasttext.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f531e74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7463 1024\n"
     ]
    }
   ],
   "source": [
    "words = model.get_words()\n",
    "print(str(len(words)) + \" \" + str(model.get_dimension()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "867291dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9927211999893188, 'expected'),\n",
       " (0.9927129149436951, 'appointed'),\n",
       " (0.992121160030365, 'called'),\n",
       " (0.9920170307159424, 'limited'),\n",
       " (0.990821897983551, 'affected'),\n",
       " (0.9905429482460022, 'pointed'),\n",
       " (0.9901823401451111, 'respected'),\n",
       " (0.9901558756828308, 'prohibited'),\n",
       " (0.9900302290916443, 'opened'),\n",
       " (0.9895544648170471, 'rejected')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick sanity check\n",
    "model.get_nearest_neighbors('the United')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff8ca7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = model.get_word_vector('great reset')\n",
    "v2 = model.get_word_vector('jew')\n",
    "v1 = model.get_word_vector('apple')\n",
    "v2 = model.get_word_vector('bench')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f41dd6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999976, 0.9881947 ],\n",
       "       [0.9881947 , 1.0000002 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([v1,v2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbd6cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the ngrams previously generated using the t-dna-ngrams.ipynb notebook\n",
    "# this file was saved as a numpy array to more easily deal with handling splitting the two columns. \n",
    "   # both pandas and I/O had issues with the spaces in the ngrams\n",
    "ngrams = np.load('../data/english_snippet_graph_matches_100k-ngrams.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51b9748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['natural immunity', 2605],\n",
       "       ['great reset', 1355],\n",
       "       ['China virus', 1185],\n",
       "       ['New York', 642],\n",
       "       ['Joe Biden', 629]], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ngrams))\n",
    "ngrams[:5]\n",
    "# [ngram, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff6de55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16919 1024\n",
      "410161\n"
     ]
    }
   ],
   "source": [
    "# generates a tsv file in the format ngram \\t count for use in T-DNA code\n",
    "# generates a numpy array of embeddings for all the ngrams for use in T-DNA code\n",
    "words = model.get_words()\n",
    "print(str(len(words)) + \" \" + str(model.get_dimension()))\n",
    "\n",
    "vectors = []\n",
    "ngrams_freq = []\n",
    "with open('../data/english_snippet_graph_matches_100k_ngrams.tsv', 'a') as the_file:\n",
    "    for w in ngrams:\n",
    "        ng= w[0]\n",
    "        v = model.get_word_vector(w[0])\n",
    "        vectors.append(v)\n",
    "        the_file.write(w[0]+'\\t'+str(w[1])+'\\n')\n",
    "        ngrams_freq.append([w[0],w[1]])\n",
    "print(len(vectors))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4b6c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../models/english_snippet_graph_matches_100k_fasttext.npy',np.array(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29046c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training data into tsv with a label column as expected by T-DNA code. Label will not be used for mlm.\n",
    "\n",
    "td = pd.read_csv('../data/english_snippet_graph_matches_100k.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b27cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snippet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shots. The people are getting Now. Cover that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's it's insane. But you know, she wants to c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>going to be honest. When I saw the tape put to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Works through all phases of illness, because i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Free People, which was the freedom to choose M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19899</th>\n",
       "      <td>the news that came out last night from Project...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19900</th>\n",
       "      <td>lasting Freedom. It answers all of the questio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19901</th>\n",
       "      <td>the questions that have to be asked. Because t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19902</th>\n",
       "      <td>most important things we can do. It's very har...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19903</th>\n",
       "      <td>settled into that. I'll be one of the people w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19904 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 snippet  label\n",
       "0      shots. The people are getting Now. Cover that ...      0\n",
       "1      It's it's insane. But you know, she wants to c...      0\n",
       "2      going to be honest. When I saw the tape put to...      0\n",
       "3      Works through all phases of illness, because i...      0\n",
       "4      Free People, which was the freedom to choose M...      0\n",
       "...                                                  ...    ...\n",
       "19899  the news that came out last night from Project...      0\n",
       "19900  lasting Freedom. It answers all of the questio...      0\n",
       "19901  the questions that have to be asked. Because t...      0\n",
       "19902  most important things we can do. It's very har...      0\n",
       "19903  settled into that. I'll be one of the people w...      0\n",
       "\n",
       "[19904 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td['label']=0\n",
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c02d441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "td.to_csv('../data/english_snippet_graph_matches_100k.tsv',sep='\\t',header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f986b",
   "metadata": {},
   "source": [
    "# Removing punctuation and stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ff776",
   "metadata": {},
   "source": [
    "removes punctuation, stopwords, and snippets shorter than 64 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afd8d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python remove_punct.py --data_path=../data/english_audio_snippets_4.4.2022.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2efbae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

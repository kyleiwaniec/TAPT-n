{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep for T-DNA\n",
    "https://github.com/shizhediao/T-DNA\n",
    "1. fasttext model from which we get ngram embeddings\n",
    "2. T-DNA expects:\n",
    "    * data in the form of `text \\t label` - english_snippet_graph_matches_100k.tsv\n",
    "    * ngrams frequency file in the form `ngram \\t count` - english_snippet_graph_matches_100k_ngrams.tsv\n",
    "    * ngram embeddings file in numpy array format - english_snippet_graph_matches_100k_fasttext.npy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Using cached fasttext-0.9.2.tar.gz (68 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pybind11>=2.2\n",
      "  Using cached pybind11-2.9.1-py2.py3-none-any.whl (211 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from fasttext) (49.6.0.post20210108)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from fasttext) (1.19.2)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=295068 sha256=d2ea0f87ef3e975ae3d9ec3167bec29028959b2f3b8669d2ab116fd464282428\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/5c/d0/4a725c6ee7df3267d818d3bc9d89bb173b94832f2b9eca6368\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.9.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model from the 'TAPT' data so we can extract warm-start embeddings for the ngrams to feed the T-DNA model training code.\n",
    "# the dimension of the vectors must be the same as the LLM we will be continuing to train. \n",
    "# since we will only be using unigrams and bigrams, we only need wordNgrams set to 2\n",
    "\n",
    "# roberta-large-dim: 1024\n",
    "# roberta-base-dim: 768\n",
    "\n",
    "languages = ['fr','de','es','hi','pt','ru','sv','tr','zh','ar']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    model = fasttext.train_unsupervised('../data/transcripts/transcripts-all-'+lang+'.csv, \n",
    "                                        model='skipgram', \n",
    "                                        lr=0.05, \n",
    "                                        dim=768, \n",
    "                                        ws=4, \n",
    "                                        wordNgrams=2, \n",
    "                                        epoch=3, \n",
    "                                        thread=12)\n",
    "    model.save_model('../models/fasttext/'+lang+'768_fasttext.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model(\"../models/fasttext/ar768_fasttext.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9350162148475647, '92%'),\n",
       " (0.9100903272628784, '1439'),\n",
       " (0.9003435373306274, '37%'),\n",
       " (0.8975725769996643, 'اثبتت'),\n",
       " (0.8953256607055664, 'انخرطت'),\n",
       " (0.8933001160621643, 'اخضرارا'),\n",
       " (0.8902260065078735, 'قمم'),\n",
       " (0.8894528746604919, '11%'),\n",
       " (0.8856233358383179, 'اصطلاحا'),\n",
       " (0.8846212029457092, 'هزيمه')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick sanity check\n",
    "model.get_nearest_neighbors('covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language: ar\n",
      "language: zh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# generates a numpy array of embeddings for all the ngrams for use in T-DNA code\n",
    "# languages = ['fr','de','es','hi','pt','ru','sv','tr']\n",
    "languages = ['ar','zh']\n",
    "for lang in languages:\n",
    "    print('language:',lang)\n",
    "    ngrams = pd.read_csv('../data/ngrams/'+lang+'_ngrams_32768.tsv',sep='\\t',names=['ngram','count'])\n",
    "    model = fasttext.load_model('../models/fasttext/'+lang+'768_fasttext.bin')\n",
    "    vectors = []\n",
    "    for row in ngrams.iterrows():\n",
    "        w = row[1]['ngram']\n",
    "        v = model.get_word_vector(w)\n",
    "        vectors.append(v)\n",
    "    np.save('../data/ngrams/'+lang+'_ngrams_32768.npy',np.array(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_word_vector('bonjour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar ['627 ../data/ngrams/ar_ngrams_32768.tsv']\n",
      "zh ['32768 ../data/ngrams/zh_ngrams_32768.tsv']\n"
     ]
    }
   ],
   "source": [
    "for lang in languages:\n",
    "    l = !wc -l ../data/ngrams/{lang}_ngrams_32768.tsv\n",
    "    print(lang, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr ['-ce\\t13566', \"aujourd'hui\\t8508\", 'peut-être\\t5146', '-là\\t4574', 'justement\\t3379', \"quelqu'\\t2957\", 'france\\t2795', \"jusqu'\\t2763\", 'demain\\t2204', 'disent\\t2106']\n",
      "de ['menschen\\t134612', 'leute\\t116217', 'sozusagen\\t50103', 'sage\\t44973', 'prozent\\t39343', 'sowas\\t31195', 'gucken\\t26793', 'irgendwas\\t26673', 'z.b.\\t25604', 'regierung\\t24950']\n",
      "es ['y a\\t16417', 'güey\\t14541', 'a a\\t8509', 'y y\\t6852', 'vas a\\t5341', 'viendo\\t4710', 'diciendo\\t4644', 'decía\\t4395', 'poquito\\t4304', 'a y\\t4219']\n",
      "hi ['दैट\\t9480', 'नॉट\\t7357', 'थिस\\t6231', 'हैव\\t5001', 'वेरी\\t4619', 'दट\\t4492', 'यू नो\\t4457', 'ऐंड\\t4189', 'पीपल\\t3773', 'बिकॉज़\\t3484']\n",
      "pt ['bolsonaro\\t42043', 'Ucrânia\\t18689', 'Estados Unidos\\t15261', 'Jovem\\t15149', 'Jovem Pan\\t14536', 'falei\\t10384', 'deu\\t9949', 'daqui\\t9515', 'Olha\\t9443', 'Jair\\t8933']\n",
      "ru ['мардан\\t2746', 'точки зрения\\t2231', 'украины\\t2083', 'Комсомольская\\t2071', 'причём\\t2023', 'Комсомольская правда\\t2000', 'радио Комсомольская\\t1943', 'россии\\t1838', 'эфире\\t1837', 'блин\\t1806']\n",
      "sv ['Jag tror\\t5825', 'allting\\t4640', 'Jag vet\\t4255', 'sån\\t4133', 'hela tiden\\t3975', 'Jag tycker\\t3757', 'utav\\t3741', 'grann\\t3483', 'okej\\t3413', 'sådär\\t3146']\n",
      "tr ['mesela\\t10242', 'Hani\\t6798', '1 şekilde\\t6264', 'evet\\t5937', \"türkiye'nin\\t5474\", 'demiş\\t4883', 'bakın\\t4864', 'hocam\\t4319', 'Mesela\\t4246', \"türkiye'de\\t4151\"]\n"
     ]
    }
   ],
   "source": [
    "for lang in languages:\n",
    "    l = !head -n 10 ../data/ngrams/{lang}_ngrams_32768.tsv\n",
    "    print(lang, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# compare embeddings from different langues. \n",
    "# We do not expect these to be similar. Just a curious though.\n",
    "\n",
    "es_model = fasttext.load_model(\"../models/fasttext/es768_fasttext.bin\")\n",
    "ru_model = fasttext.load_model(\"../models/fasttext/ru768_fasttext.bin\")\n",
    "de_model = fasttext.load_model(\"../models/fasttext/de768_fasttext.bin\")\n",
    "fr_model = fasttext.load_model(\"../models/fasttext/fr768_fasttext.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['quizás','может быть','vielleicht','peut-être']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_text=es_model.get_word_vector(text[0])\n",
    "ru_text=ru_model.get_word_vector(text[1])\n",
    "de_text=de_model.get_word_vector(text[2])\n",
    "fr_text=fr_model.get_word_vector(text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_sim(a, b):\n",
    "\t\"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "\tto the definition of the dot product\n",
    "\t\"\"\"\n",
    "\tdot_product = np.dot(a, b)\n",
    "\tnorm_a = np.linalg.norm(a)\n",
    "\tnorm_b = np.linalg.norm(b)\n",
    "\treturn dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023469687\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(sp_text, ru_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015318886\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(sp_text, de_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04430275\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(sp_text, fr_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.020328093\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(ru_text, de_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.097885266\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(ru_text, fr_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012663008\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(de_text, fr_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

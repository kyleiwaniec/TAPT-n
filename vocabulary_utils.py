import pandas as pd
import numpy as np
from collections import defaultdict
import fasttext
import json
import matplotlib.pyplot as plt

import spacy
from spacy.lang.en import English
from collections import Counter
import heapq
from tqdm import tqdm


def getNgrams(data,
               stopwords=None,
               spacy_model=None,
               language=None,
               LLMvocab=None,
               max_n=32768,
               pmi=True):
    '''
    data: list or Series of strings, ex) df['snippet']
    returns list of ngrams and counts
    '''
        
    vocab_dict = defaultdict(lambda: [0,0,[0]])
    bigram_dict = defaultdict(lambda: [0,0])
    
    _puntuation = [".", "?", "”", ",", "-", "–", "—", "!", ":", ";", "(", ")", "[", "]", "…", "/"]
    _stopwords = []
    if stopwords:
        with open('../data/all-transcripts/stopwords/'+stopwords, 'r') as _f:
            for line in _f.readlines():
                _stopwords.append(line.strip())
    
    t = tqdm(data)
    doc_id, vocab_len = 0,0
    
    for text in t:
        t.set_description("Counting ngrams", refresh=True)
        prev_word = None
        
        doc_id += 1
        _text = text.strip().split(" ")
        for token in _text: 
            if token not in _puntuation and token not in _stopwords:    
                cur_word = token.text.strip()
                if prev_word:
                    bigram_dict[prev_word+" "+cur_word][0] += 1
                    bigram_dict[prev_word+" "+cur_word][1] = float('-inf')
                
                vocab_dict[cur_word][0] +=1
                vocab_dict[cur_word][1] = float('-inf')
                vocab_dict[cur_word][2].append(doc_id) # for caculating tfidf
                
                prev_word=cur_word
                vocab_len += 1

    num_documents = doc_id
        
    vocab_dict = calculateTFIDF(vocab_dict,num_documents) if pmi else vocab_dict
    bigram_dict = calculatePMI(vocab_dict,bigram_dict,vocab_len) if pmi else bigram_dict
    
    vocab_set = set(vocab_dict.keys())
    common_vocab = vocab_set.intersection(set(LLMvocab))
    
    # remove words which are already in the LLM vocabulary
    for w in list(common_vocab):
        del vocab_dict[w]
    
    # combine unigrams with bigrams
    combined_dict = dict(bigram_dict, **vocab_dict)
    
    # sort and convert to list
    combined_list = sorted(combined_dict.items(), key=lambda x: -x[1][1]) if pmi else sorted(combined_dict.items(), key=lambda x: -x[1][0])
    
    # remove redundant columns
    combined_list = [(x[0],x[1][1]) for x in combined_list] if pmi else [(x[0],x[1][0]) for x in combined_list]
    
    # return max_n results
    return combined_list if len(combined_list) < max_n else combined_list[:max_n], common_vocab

def getNgramsSpacy(data,
                   stopwords=None,
                   spacy_model='en_core_web_sm',
                   language=None,
                   LLMvocab=None,
                   max_n=32768,
                   pmi=True):
    '''
    data: list or Series of strings, ex) df['snippet']
    LLMvocab: list containing the LLM vocabulary
    pmi: calculates and orders by pmi (pointwise mutual information). If false, uses frequencies instead.
    returns: list of tuples of (ngram,count) up to a maximum length of max_n, and a set of the common words between the domain and the LLM
    '''
    
    nlp = spacy.load(spacy_model)
    
    # We keep separate dictionaries for unigrams and bigrams  
    # to make it easier to calculate PMI later.
    vocab_dict = defaultdict(lambda: [0,0,[0]])
    bigram_dict = defaultdict(lambda: [0,0])
    
    _stopwords = []
    if stopwords:
        with open('../data/all-transcripts/stopwords/'+stopwords, 'r') as _f:
            for line in _f.readlines():
                _stopwords.append(line.strip())
    
    t = tqdm(data)
    doc_id, vocab_len = 0,0
    
    for text in t:
        t.set_description("Counting ngrams", refresh=True)
        prev_word = None
        
        doc_id += 1
        
        #replace carriage returns - otherwise the number of ngrams generated by fasttext won't match.
        text = str(text).replace('\n',' ').replace('\\n', '')
        _text = nlp(text)
        
        for token in _text: 
            if not token.is_punct and not token.is_stop and token.text not in _stopwords:
                cur_word = token.text.strip()
                if prev_word:
                    bigram_dict[prev_word+" "+cur_word][0] += 1
                    bigram_dict[prev_word+" "+cur_word][1] = float('-inf')
                
                vocab_dict[cur_word][0] +=1
                vocab_dict[cur_word][1] = float('-inf')
                vocab_dict[cur_word][2].append(doc_id) # for caculating tfidf
                
                prev_word=cur_word
                vocab_len += 1

    num_documents = doc_id
        
    vocab_dict = calculateTFIDF(vocab_dict,num_documents) if pmi else vocab_dict
    bigram_dict = calculatePMI(vocab_dict,bigram_dict,vocab_len) if pmi else bigram_dict
    
    vocab_set = set(vocab_dict.keys())
    common_vocab = vocab_set.intersection(set(LLMvocab))
    
    # remove words which are already in the LLM vocabulary
    for w in list(common_vocab):
        del vocab_dict[w]
    
    # combine unigrams with bigrams
    combined_dict = dict(bigram_dict, **vocab_dict)
    
    # sort and convert to list
    combined_list = sorted(combined_dict.items(), key=lambda x: -x[1][1]) if pmi else sorted(combined_dict.items(), key=lambda x: -x[1][0])
    
    # remove redundant columns
    combined_list = [(x[0],x[1][1]) for x in combined_list] if pmi else [(x[0],x[1][0]) for x in combined_list]
    
    # return max_n results
    return combined_list if len(combined_list) < max_n else combined_list[:max_n], common_vocab
    

def calculateTFIDF(vocab_dict,N):
    
    # tfidf = term_count_in_doc * log(num_documents/num_docs_with_term)
    # for our purposes we'll use the average term count in document - it's sorta handwavey, but let's see if it works
    
    t = tqdm(vocab_dict.items())
    for key, val in t:
        t.set_description("Calculating TFIDF", refresh=True)
        
        num_documents = N
        num_docs_with_term = len(set(val[2]))
        
        ############## The average term count. ############## 
        '''
        Ex) 
        val[2] = [0,0,1,2,2,2,3] <- list of document ids where the term occurs
        counts.values() = [2, 1, 3, 1] <- number of occurances of each doc_id
        len(counts.values() = [4] <- number of unique documents
        avg_term_count_in_doc = (2+1+3+1)/4 = 1.75 <- average term count in document
        '''
        
        counts = Counter(val[2])
        avg_term_count_in_doc = sum(counts.values())/len(counts.values())
        
        avg_tfidf = avg_term_count_in_doc * np.log(num_documents/num_docs_with_term)
        
        vocab_dict[key][1] = avg_tfidf
      
    return vocab_dict
    
def calculatePMI(vocab_dict,bigram_dict,total):
    # pmi(a;b) = ln(p(a,b)/(p(a)*p(b)))
                
    t = tqdm(bigram_dict.items())
    for key, val in t:
        t.set_description("Calculating PMI", refresh=True)
        a,b = key.split(' ')
        
        p_a = vocab_dict[a][0]/total
        p_b = vocab_dict[b][0]/total
        p_ab = val[0]/total

        pmi = np.log(p_ab/(p_a*p_b))

        bigram_dict[key][1] = pmi
        
    return bigram_dict


def plotDistribution(vocab_list):
    vocab_list = np.array(vocab_list)
#     plt.xticks([])
    plt.title("Word frequency distribution")
    plt.plot(vocab_list[:,0],vocab_list[:,1].astype(int))
    plt.show()
    
    
def displayResults(path_to_data,vocab_dict,new_vocab_list,n):
    print(path_to_data)
    print("\nSize of vocabulary:",f"{len(vocab_dict):,}")
    print("\nNumber of new words:",f"{len(new_vocab_list):,}")
    print("\nTop ",n," words\n",new_vocab_list[:n])
    print("\nBottom ",n, " words\n",new_vocab_list[-n:])
    plotDistribution(new_vocab_list)